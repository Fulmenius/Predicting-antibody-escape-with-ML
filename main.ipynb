{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "bc7f6967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot_LSTM is loaded\n",
      "one_hot_CNN is loaded\n",
      "one_hot_XGBoost is loaded\n",
      "ProtT5_CNN is loaded\n",
      "ProtT5_XGBoost is loaded\n",
      "Training routine is loaded\n",
      "utils are loaded\n"
     ]
    }
   ],
   "source": [
    "#!pip install xgboost\n",
    "\n",
    "# Importing models and training loop\n",
    "%run \"./models/one_hot_LSTM.ipynb\"\n",
    "%run \"./models/one_hot_CNN.ipynb\"\n",
    "%run \"./models/one_hot_XGBoost.ipynb\"\n",
    "%run \"./models/ProtT5_CNN.ipynb\"\n",
    "%run \"./models/ProtT5_XGBoost.ipynb\"\n",
    "%run \"./models/training_routine.ipynb\"\n",
    "%run \"./models/utils.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "42f7c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Importing data\n",
    "ACE2_train = pd.read_csv(\"./data/ACE2_train_data.csv\")\n",
    "ACE2_test = pd.read_csv(\"./data/ACE2_test_data.csv\")\n",
    "LY16_train = pd.read_csv(\"./data/LY16_train_data.csv\")\n",
    "LY16_test = pd.read_csv(\"./data/LY16_test_data.csv\")\n",
    "LY555_train = pd.read_csv(\"./data/LY555_test_data.csv\")\n",
    "LY555_test = pd.read_csv(\"./data/LY555_train_data.csv\")\n",
    "REGN33_train = pd.read_csv(\"./data/REGN33_train_data.csv\")\n",
    "REGN33_test = pd.read_csv(\"./data/REGN33_test_data.csv\")\n",
    "REGN87_train = pd.read_csv(\"./data/REGN87_train_data.csv\")\n",
    "REGN87_test = pd.read_csv(\"./data/REGN87_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "2b6bca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_antibodies = [LY16_train, LY555_train, REGN33_train, REGN87_train]\n",
    "test_antibodies = [LY16_test, LY555_test, REGN33_test, REGN87_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "678968c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General settings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "07e6d36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 0.6286\n",
      "Epoch 2/10, Training Loss: 0.4896\n",
      "Epoch 3/10, Training Loss: 0.4137\n",
      "Epoch 4/10, Training Loss: 0.3632\n",
      "Epoch 5/10, Training Loss: 0.3205\n",
      "Epoch 6/10, Training Loss: 0.2923\n",
      "Epoch 7/10, Training Loss: 0.2599\n",
      "Epoch 8/10, Training Loss: 0.2107\n",
      "Epoch 9/10, Training Loss: 0.1978\n",
      "Epoch 10/10, Training Loss: 0.1852\n",
      "ROC/AUC Score: 0.8754\n",
      "Epoch 1/10, Training Loss: 0.4582\n",
      "Epoch 2/10, Training Loss: 0.3375\n",
      "Epoch 3/10, Training Loss: 0.2846\n",
      "Epoch 4/10, Training Loss: 0.2509\n",
      "Epoch 5/10, Training Loss: 0.2178\n",
      "Epoch 6/10, Training Loss: 0.1933\n",
      "Epoch 7/10, Training Loss: 0.1624\n",
      "Epoch 8/10, Training Loss: 0.1365\n",
      "Epoch 9/10, Training Loss: 0.1391\n",
      "Epoch 10/10, Training Loss: 0.1366\n",
      "ROC/AUC Score: 0.8747\n",
      "Epoch 1/10, Training Loss: 0.4450\n",
      "Epoch 2/10, Training Loss: 0.3090\n",
      "Epoch 3/10, Training Loss: 0.2431\n",
      "Epoch 4/10, Training Loss: 0.2036\n",
      "Epoch 5/10, Training Loss: 0.1738\n",
      "Epoch 6/10, Training Loss: 0.1466\n",
      "Epoch 7/10, Training Loss: 0.1313\n",
      "Epoch 8/10, Training Loss: 0.1046\n",
      "Epoch 9/10, Training Loss: 0.0999\n",
      "Epoch 10/10, Training Loss: 0.0817\n",
      "ROC/AUC Score: 0.8735\n",
      "Epoch 1/10, Training Loss: 0.4940\n",
      "Epoch 2/10, Training Loss: 0.3264\n",
      "Epoch 3/10, Training Loss: 0.2610\n",
      "Epoch 4/10, Training Loss: 0.2167\n",
      "Epoch 5/10, Training Loss: 0.1863\n",
      "Epoch 6/10, Training Loss: 0.1633\n",
      "Epoch 7/10, Training Loss: 0.1406\n",
      "Epoch 8/10, Training Loss: 0.1299\n",
      "Epoch 9/10, Training Loss: 0.1130\n",
      "Epoch 10/10, Training Loss: 0.1024\n",
      "ROC/AUC Score: 0.8933\n",
      "Epoch 1/10, Training Loss: 0.4450\n",
      "Epoch 2/10, Training Loss: 0.3010\n",
      "Epoch 3/10, Training Loss: 0.2251\n",
      "Epoch 4/10, Training Loss: 0.2014\n",
      "Epoch 5/10, Training Loss: 0.1659\n",
      "Epoch 6/10, Training Loss: 0.1406\n",
      "Epoch 7/10, Training Loss: 0.1233\n",
      "Epoch 8/10, Training Loss: 0.1004\n",
      "Epoch 9/10, Training Loss: 0.1021\n",
      "Epoch 10/10, Training Loss: 0.0812\n",
      "ROC/AUC Score: 0.8948\n",
      "Epoch 1/10, Training Loss: 1.2177\n",
      "Epoch 2/10, Training Loss: 0.4756\n",
      "Epoch 3/10, Training Loss: 0.3737\n",
      "Epoch 4/10, Training Loss: 0.3182\n",
      "Epoch 5/10, Training Loss: 0.2748\n",
      "Epoch 6/10, Training Loss: 0.2307\n",
      "Epoch 7/10, Training Loss: 0.2102\n",
      "Epoch 8/10, Training Loss: 0.1912\n",
      "Epoch 9/10, Training Loss: 0.1690\n",
      "Epoch 10/10, Training Loss: 0.1486\n",
      "ROC/AUC Score: 0.9067\n",
      "Epoch 1/10, Training Loss: 0.2683\n",
      "Epoch 2/10, Training Loss: 0.1763\n",
      "Epoch 3/10, Training Loss: 0.1427\n",
      "Epoch 4/10, Training Loss: 0.1248\n",
      "Epoch 5/10, Training Loss: 0.1027\n",
      "Epoch 6/10, Training Loss: 0.0910\n",
      "Epoch 7/10, Training Loss: 0.0896\n",
      "Epoch 8/10, Training Loss: 0.0671\n",
      "Epoch 9/10, Training Loss: 0.0672\n",
      "Epoch 10/10, Training Loss: 0.0498\n",
      "ROC/AUC Score: 0.8969\n",
      "Epoch 1/10, Training Loss: 0.1892\n",
      "Epoch 2/10, Training Loss: 0.1302\n",
      "Epoch 3/10, Training Loss: 0.0902\n",
      "Epoch 4/10, Training Loss: 0.0829\n",
      "Epoch 5/10, Training Loss: 0.0630\n",
      "Epoch 6/10, Training Loss: 0.0531\n",
      "Epoch 7/10, Training Loss: 0.0426\n",
      "Epoch 8/10, Training Loss: 0.0508\n",
      "Epoch 9/10, Training Loss: 0.0449\n",
      "Epoch 10/10, Training Loss: 0.0551\n",
      "ROC/AUC Score: 0.9202\n",
      "Epoch 1/10, Training Loss: 0.1607\n",
      "Epoch 2/10, Training Loss: 0.0895\n",
      "Epoch 3/10, Training Loss: 0.0573\n",
      "Epoch 4/10, Training Loss: 0.0540\n",
      "Epoch 5/10, Training Loss: 0.0457\n",
      "Epoch 6/10, Training Loss: 0.0333\n",
      "Epoch 7/10, Training Loss: 0.0372\n",
      "Epoch 8/10, Training Loss: 0.0266\n",
      "Epoch 9/10, Training Loss: 0.0242\n",
      "Epoch 10/10, Training Loss: 0.0216\n",
      "ROC/AUC Score: 0.9153\n",
      "Epoch 1/10, Training Loss: 0.1173\n",
      "Epoch 2/10, Training Loss: 0.0672\n",
      "Epoch 3/10, Training Loss: 0.0492\n",
      "Epoch 4/10, Training Loss: 0.0340\n",
      "Epoch 5/10, Training Loss: 0.0287\n",
      "Epoch 6/10, Training Loss: 0.0559\n",
      "Epoch 7/10, Training Loss: 0.0447\n",
      "Epoch 8/10, Training Loss: 0.0267\n",
      "Epoch 9/10, Training Loss: 0.0221\n",
      "Epoch 10/10, Training Loss: 0.0149\n",
      "ROC/AUC Score: 0.9261\n",
      "Epoch 1/10, Training Loss: 1.5430\n",
      "Epoch 2/10, Training Loss: 0.6599\n",
      "Epoch 3/10, Training Loss: 0.4632\n",
      "Epoch 4/10, Training Loss: 0.3360\n",
      "Epoch 5/10, Training Loss: 0.2724\n",
      "Epoch 6/10, Training Loss: 0.2230\n",
      "Epoch 7/10, Training Loss: 0.1984\n",
      "Epoch 8/10, Training Loss: 0.1585\n",
      "Epoch 9/10, Training Loss: 0.1328\n",
      "Epoch 10/10, Training Loss: 0.1091\n",
      "ROC/AUC Score: 0.8535\n",
      "Epoch 1/10, Training Loss: 0.6759\n",
      "Epoch 2/10, Training Loss: 0.3637\n",
      "Epoch 3/10, Training Loss: 0.2506\n",
      "Epoch 4/10, Training Loss: 0.1963\n",
      "Epoch 5/10, Training Loss: 0.1484\n",
      "Epoch 6/10, Training Loss: 0.1271\n",
      "Epoch 7/10, Training Loss: 0.1124\n",
      "Epoch 8/10, Training Loss: 0.0944\n",
      "Epoch 9/10, Training Loss: 0.0756\n",
      "Epoch 10/10, Training Loss: 0.0639\n",
      "ROC/AUC Score: 0.8925\n",
      "Epoch 1/10, Training Loss: 0.6327\n",
      "Epoch 2/10, Training Loss: 0.3677\n",
      "Epoch 3/10, Training Loss: 0.2650\n",
      "Epoch 4/10, Training Loss: 0.1917\n",
      "Epoch 5/10, Training Loss: 0.1603\n",
      "Epoch 6/10, Training Loss: 0.1322\n",
      "Epoch 7/10, Training Loss: 0.1052\n",
      "Epoch 8/10, Training Loss: 0.0955\n",
      "Epoch 9/10, Training Loss: 0.0790\n",
      "Epoch 10/10, Training Loss: 0.0643\n",
      "ROC/AUC Score: 0.9034\n",
      "Epoch 1/10, Training Loss: 0.5413\n",
      "Epoch 2/10, Training Loss: 0.3244\n",
      "Epoch 3/10, Training Loss: 0.2232\n",
      "Epoch 4/10, Training Loss: 0.1704\n",
      "Epoch 5/10, Training Loss: 0.1286\n",
      "Epoch 6/10, Training Loss: 0.1175\n",
      "Epoch 7/10, Training Loss: 0.0984\n",
      "Epoch 8/10, Training Loss: 0.0766\n",
      "Epoch 9/10, Training Loss: 0.0703\n",
      "Epoch 10/10, Training Loss: 0.0584\n",
      "ROC/AUC Score: 0.9280\n",
      "Epoch 1/10, Training Loss: 0.5293\n",
      "Epoch 2/10, Training Loss: 0.3006\n",
      "Epoch 3/10, Training Loss: 0.2319\n",
      "Epoch 4/10, Training Loss: 0.1841\n",
      "Epoch 5/10, Training Loss: 0.1448\n",
      "Epoch 6/10, Training Loss: 0.1261\n",
      "Epoch 7/10, Training Loss: 0.1040\n",
      "Epoch 8/10, Training Loss: 0.0861\n",
      "Epoch 9/10, Training Loss: 0.0871\n",
      "Epoch 10/10, Training Loss: 0.0815\n",
      "ROC/AUC Score: 0.9129\n",
      "Epoch 1/10, Training Loss: 0.5604\n",
      "Epoch 2/10, Training Loss: 0.2558\n",
      "Epoch 3/10, Training Loss: 0.1484\n",
      "Epoch 4/10, Training Loss: 0.1113\n",
      "Epoch 5/10, Training Loss: 0.0908\n",
      "Epoch 6/10, Training Loss: 0.0808\n",
      "Epoch 7/10, Training Loss: 0.0588\n",
      "Epoch 8/10, Training Loss: 0.0574\n",
      "Epoch 9/10, Training Loss: 0.0434\n",
      "Epoch 10/10, Training Loss: 0.0418\n",
      "ROC/AUC Score: 0.9789\n",
      "Epoch 1/10, Training Loss: 0.2151\n",
      "Epoch 2/10, Training Loss: 0.1017\n",
      "Epoch 3/10, Training Loss: 0.0666\n",
      "Epoch 4/10, Training Loss: 0.0633\n",
      "Epoch 5/10, Training Loss: 0.0532\n",
      "Epoch 6/10, Training Loss: 0.0326\n",
      "Epoch 7/10, Training Loss: 0.0297\n",
      "Epoch 8/10, Training Loss: 0.0273\n",
      "Epoch 9/10, Training Loss: 0.0259\n",
      "Epoch 10/10, Training Loss: 0.0204\n",
      "ROC/AUC Score: 0.9763\n",
      "Epoch 1/10, Training Loss: 0.1759\n",
      "Epoch 2/10, Training Loss: 0.0963\n",
      "Epoch 3/10, Training Loss: 0.0712\n",
      "Epoch 4/10, Training Loss: 0.0619\n",
      "Epoch 5/10, Training Loss: 0.0585\n",
      "Epoch 6/10, Training Loss: 0.0371\n",
      "Epoch 7/10, Training Loss: 0.0236\n",
      "Epoch 8/10, Training Loss: 0.0362\n",
      "Epoch 9/10, Training Loss: 0.0169\n",
      "Epoch 10/10, Training Loss: 0.0197\n",
      "ROC/AUC Score: 0.9789\n",
      "Epoch 1/10, Training Loss: 0.2264\n",
      "Epoch 2/10, Training Loss: 0.1192\n",
      "Epoch 3/10, Training Loss: 0.0906\n",
      "Epoch 4/10, Training Loss: 0.0596\n",
      "Epoch 5/10, Training Loss: 0.0499\n",
      "Epoch 6/10, Training Loss: 0.0469\n",
      "Epoch 7/10, Training Loss: 0.0429\n",
      "Epoch 8/10, Training Loss: 0.0313\n",
      "Epoch 9/10, Training Loss: 0.0283\n",
      "Epoch 10/10, Training Loss: 0.0269\n",
      "ROC/AUC Score: 0.9723\n",
      "Epoch 1/10, Training Loss: 0.2469\n",
      "Epoch 2/10, Training Loss: 0.1587\n",
      "Epoch 3/10, Training Loss: 0.1154\n",
      "Epoch 4/10, Training Loss: 0.0827\n",
      "Epoch 5/10, Training Loss: 0.0709\n",
      "Epoch 6/10, Training Loss: 0.0494\n",
      "Epoch 7/10, Training Loss: 0.0457\n",
      "Epoch 8/10, Training Loss: 0.0411\n",
      "Epoch 9/10, Training Loss: 0.0340\n",
      "Epoch 10/10, Training Loss: 0.0331\n",
      "ROC/AUC Score: 0.9728\n",
      "CNN model:\n",
      "LY16: [0.8754197837738312, 0.8747178871548621, 0.8734914456920448, 0.8932918653009434, 0.8948304239701094]\n",
      "LY555: [0.9067171832957274, 0.8968787515006003, 0.9202352950604867, 0.9153410822865026, 0.9261253405122585]\n",
      "REGN33: [0.8535329883000911, 0.8925045206509736, 0.903420523138833, 0.9279605315891566, 0.912874605993696]\n",
      "REGN87: [0.9788955582232893, 0.9763094168965295, 0.9789316449815987, 0.9723080494364754, 0.9727777777777776]\n",
      "XGBoost model:\n",
      "LY16: [0.5190708919229399, 0.4831132452981193, 0.4998099931597538, 0.49324143120749625, 0.4848404755897818]\n",
      "LY555: [0.5329826127876249, 0.5077270908363345, 0.5126543222244039, 0.517090211933067, 0.5304190950874231]\n",
      "REGN33: [0.5029920226072825, 0.46097438031076476, 0.49095767447628114, 0.5034386572404174, 0.5247243955903295]\n",
      "REGN87: [0.4817927170868348, 0.5178210597215985, 0.49228508155427675, 0.5112144595286885, 0.5469283413848631]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "input_dim = 20  # number of unique characters in our sequences\n",
    "output_dim = 1\n",
    "\n",
    "antibodies = [\"LY16\", \"LY555\", \"REGN33\", \"REGN87\"]\n",
    "models = {\n",
    "    \"CNN\": ConvNet(),\n",
    "    \"XGBoost\": xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "}\n",
    "results = {model: {antibody: [] for antibody in antibodies} for model in models}\n",
    "\n",
    "for i, (train_antibody, test_antibody) in enumerate(zip(train_antibodies, test_antibodies)):\n",
    "    for _ in range(5):\n",
    "        train_loader = prepare_data(train_antibody, sample_size=1000)\n",
    "        test_loader = prepare_data(test_antibody, sample_size=1000)\n",
    "        for model_name, model in models.items():\n",
    "            if model_name == \"XGBoost\":\n",
    "                # Flatten the sequences\n",
    "                X_train = torch.vstack([x for x, y in train_loader]).reshape(-1, 24*20).numpy()\n",
    "                y_train = torch.hstack([y for x, y in train_loader]).numpy()\n",
    "                X_test = torch.vstack([x for x, y in test_loader]).reshape(-1, 24*20).numpy()\n",
    "                y_test = torch.hstack([y for x, y in test_loader]).numpy()\n",
    "\n",
    "                # Train the XGBoost model\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Predict on test set\n",
    "                y_pred = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "                # Calculate ROC/AUC score\n",
    "                score = roc_auc_score(y_test, y_pred)\n",
    "            else:\n",
    "                # Handle other models (CNN) similarly\n",
    "                criterion = nn.BCEWithLogitsLoss()\n",
    "                learning_rate = 1e-3\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                model = model.to(device)\n",
    "                score = train_and_evaluate(model, train_loader, test_loader, criterion, optimizer)\n",
    "            results[model_name][antibodies[i]].append(score)\n",
    "\n",
    "# Print the results\n",
    "for model, scores in results.items():\n",
    "    print(f\"{model} model:\")\n",
    "    for antibody, score in scores.items():\n",
    "        print(f\"{antibody}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb620c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# Create boxplot\n",
    "fig, ax = plt.subplots()\n",
    "model_names = list(results.keys())\n",
    "data = [results[model_name][antibody] for model_name in model_names for antibody in antibodies]\n",
    "ax.boxplot(data, labels=[f\"{model_name}\\n{antibody}\" for model_name in model_names for antibody in antibodies])\n",
    "\n",
    "plt.title('ROC/AUC Score Comparison')\n",
    "plt.ylabel('ROC/AUC Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "8929a4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/paul-valery/miniconda3/lib/python3.9/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/paul-valery/miniconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m486.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/paul-valery/miniconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: filelock in /home/paul-valery/miniconda3/lib/python3.9/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/paul-valery/miniconda3/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/paul-valery/miniconda3/lib/python3.9/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/paul-valery/miniconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/paul-valery/miniconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m161.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /home/paul-valery/miniconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/paul-valery/miniconda3/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/paul-valery/miniconda3/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/paul-valery/miniconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Installing collected packages: tokenizers, fsspec, huggingface-hub, transformers\n",
      "Successfully installed fsspec-2023.5.0 huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413a1e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40fadec976ef41378638c9d8172c58c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a14159b92524a97b9badac6f7273b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transformer_link = \"Rostlab/prot_t5_xl_half_uniref50-enc\"\n",
    "model_t5 = T5EncoderModel.from_pretrained(transformer_link)\n",
    "model_t5.full() if device=='cpu' else model_t5.half() \n",
    "model_t5 = model_t5.to(device)\n",
    "model_t5 = model_t5.eval()\n",
    "tokenizer = T5Tokenizer.from_pretrained(transformer_link, do_lower_case=False )\n",
    "\n",
    "def preprocess_sequences(sequences):\n",
    "    sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequences]\n",
    "    embeddings = batch_process_sequences(model_t5, tokenizer, sequence_examples, batch_size=20)\n",
    "    return embeddings\n",
    "\n",
    "def prepare_data_with_embedding(antibody, sample_size):\n",
    "    # Select a subset of data\n",
    "    subset = antibody.sample(sample_size)\n",
    "    sequences = subset[\"junction_aa\"]\n",
    "    labels = subset[\"binds\"]\n",
    "    \n",
    "    # Preprocess the sequences into embeddings\n",
    "    embeddings = preprocess_sequences(sequences)\n",
    "    \n",
    "    # Create a TensorDataset from the embeddings and labels\n",
    "    dataset = torch.utils.data.TensorDataset(embeddings, torch.Tensor(labels.values))\n",
    "    \n",
    "    # Create a DataLoader\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "input_dim = 1024  # number of features in ProtT5 embeddings\n",
    "output_dim = 1\n",
    "\n",
    "antibodies = [\"LY16\", \"LY555\", \"REGN33\", \"REGN87\"]\n",
    "models = {\n",
    "    \"CNN\": ConvNet(input_dim, output_dim),\n",
    "    \"XGBoost\": xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "}\n",
    "results = {model: {antibody: [] for antibody in antibodies} for model in models}\n",
    "\n",
    "for i, (train_antibody, test_antibody) in enumerate(zip(train_antibodies, test_antibodies)):\n",
    "    for _ in range(5):\n",
    "        train_loader = prepare_data_with_embedding(train_antibody, sample_size=1000)\n",
    "        test_loader = prepare_data_with_embedding(test_antibody, sample_size=1000)\n",
    "        for model_name, model in models.items():\n",
    "            if model_name == \"XGBoost\":\n",
    "                # Flatten the embeddings\n",
    "                X_train = torch.vstack([x for x, y in train_loader]).reshape(-1, input_dim).numpy()\n",
    "                y_train = torch.hstack([y for x, y in train_loader]).numpy()\n",
    "                X_test = torch.vstack([x for x, y in test_loader]).reshape(-1, input_dim).numpy()\n",
    "                y_test = torch.hstack([y for x, y in test_loader]).numpy()\n",
    "\n",
    "                # Train the XGBoost model\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Predict on test set\n",
    "                y_pred = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "                # Calculate ROC/AUC score\n",
    "                score = roc_auc_score(y_test, y_pred)\n",
    "            else:\n",
    "                criterion = nn.BCEWithLogitsLoss()\n",
    "                learning_rate = 1e-3\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                model = model.to(device)\n",
    "                score = train_and_evaluate(model, train_loader, test_loader, criterion, optimizer)\n",
    "            results[model_name][antibodies[i]].append(score)\n",
    "\n",
    "# Print the results\n",
    "for model, scores in results.items():\n",
    "    print(f\"{model} model:\")\n",
    "    for antibody, score in scores.items():\n",
    "        print(f\"{antibody}: {score}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create boxplot\n",
    "fig, ax = plt.subplots()\n",
    "model_names = list(results.keys())\n",
    "data = [results[model_name][antibody] for model_name in model_names for antibody in antibodies]\n",
    "ax.boxplot(data, labels=[f\"{model_name}\\n{antibody}\" for model_name in model_names for antibody in antibodies])\n",
    "\n",
    "plt.title('ROC/AUC Score Comparison')\n",
    "plt.ylabel('ROC/AUC Score')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
