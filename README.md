# Predicting-antibody-escape-with-ML
Predicting antibody escape and ACE2 binding of COVID-19 spike protein based on the amino-acid sequence of RBD

## Introduction
At the early stages of the COVID-19 pandemic, monoclonal antibodies were probably the only effective non-symptomatic treatment of the disease.[] Later on, the performance of vaccines against the coronavirus was estimated, among other things, from the production of antibodies against the virus after vaccination, as they were considered to be one of the key mechanisms by which vaccination prevents and/or lightens the infection.[] However, COVID-19 mutates fairly quickly, and some of the new strains develop the ability to escape antibodies effective against the original Wuhan strain, both obtained the "natural" way and with the help of the vaccines.[] This process may render some of the current protocols for treatment and prevention of the disease outdated, and hence poses a serious threat to the world health security. Hence, it would be extremely useful to know in advance, without having to perform the tests _in vitro_, what possible mutations of the coronavirus may help it escape the immunity created by vaccines and infections with older strains.

This is the problem that Taft et al. addressed in their paper [T]. Using both classical Machine Learning and Deep Learning models, and a vast database of mutation effects obtained via yeast surface display [Y], they were able to predict the effect of both singular and combinatorial mutations on the ability of the four antibodies used in clinical practice to bind to a mutated receptor-binding domain (RBD) of SARS-CoV 2 S-protein, with precision exceeding 90% and reaching 100% in some problems. 

This project's aim is twofold. At one hand, it is a simple reproduction of results of Taft et al. At the other, there is some hope of improving these results by using more advanced Machine Learning techniques. Taft et al. used one-hot encoding as a vector representation for the amino-acid sequence of the mutated region of RBD. This representation does not represent well the chemical and biological structure of the space of amino acid sequences. A better way to do that is provided by the use of autoencoders - userupervisedly learned vector representations of data, that have been used with great success in the field of Natural Language Processing. Such models can learn grammar of natural languages and represent it using the spatial structure of the vector representation. Elnaggar et al.[PT], among others, used state of the art autoencoders and auto-regressive models, such as Transformers, to create vector representations for protein sequences that reflect the chemical and biological structure of the "protein space", i.e., the "grammar" of the "language of life". Using these pretrained encodings as inputs for other models, a technique called Transfer Learning, they managed to significantly improve the performance of these models. The ultimate goal of our project is to do the same in the context of work of Taft et al.

## Formulation of the problem and the data
Antibodies are proteins produced by immune system to bind to specific molecules, normally those foreign to the organism. Each antibody is tailored to "recognize" a specific molecule, called the _antigene_ of the antibody. By binding their respective antigenes, antibodies mark them for destruction by other components of the immune system, and sometimes physically prevent the antigenes from performing their normal functions. If the antigene is large, like protein, the antibody usually recognizes only a certain part of the antigene, called _epitope_. In case of SARS CoV 2, one of the most effective choices of antigenes for the immune system to react to is its so-called spike protein (S-protein), which the coronavirus uses to bind to a specific human receptor protein embedded into the membrane of certain types of human cells - the ACE2 receptor. The part of the coronavirus S-protein which directly interacts with the ACE2 receptor is thus called the receptor-binding domain, or RBD for short. Binding to the RBD and physically blocking it, an antibody is able not only to mark the viral particle for destruction, but also prevent infection of human cells. Thus, antibodies which have the RBD as their epitope are used in clinical practice for coronavirus treatment. 

Each protein is a long chain of 20 elementary building blocks, (alpha-)amino acids (or a complex of such chains). 
Since binding of antibodies to their epitopes is highly specific, a random mutation in the amino-acid chain of the RBD can enable the coronavirus to escape such antibodies. However, a mutation can also hinder the ability of RBD to bind to the ACE2 receptor, making the virus unable to reproduce. Thus, a potentially dangerous mutation is such that enables the virus to escape antibodies, while preserving its ability to bind to ACE2 receptors. Our goal is to predict both based on the amino acid sequence of the mutated RBD. More specifically, we use one of subsequences of the RBD which are particularly important to ACE2 binding (called core regions of the receptor-binding motif), RBM-2, 24 amino acids long. 

The data we are given with consists of 5 datasets, containing information on binding of ACE2 receptors and four kinds of antibodies used in clinical practice to the mutated RBD: LY16, LY555, REGN33 and REGN87, together with the corresponding mutated RBM-2 sequences.
The mutated sequeces of RBM-2 are represented as strings of capital letters of the latin alphabet 24 characters long. Binding or non-binding is represented with binary labels (1 for binding, 0 for non-binding). The datasets also contain edit distances from the mutated RBM-2s to the reference RBM-2 of the original Wuhan strain. There are other data fields, like "consensus count", which we are not interested in. 

Information on binding of mutated RBDs is obtained experimentally by Taft et al., via technique called yeast surface display - the detailed description of experimental data acquisition can be found in the original publication [T]. The datasets used in this work, also created by Taft et al., can be found at https://github.com/LSSI-ETH/Taft_Weber_2021/blob/main/Supp.%20Table%204%20Model_Sequences.zip. 

All five datasets were splitted by the authors of the original paper in test, train and validate, with all three splits being balanced by the number of binding and non-binding variants. The train dataset sizes vary from about 15,000 entries (LY555) to about 407,000 entries (ACE2). The train/test split size ratio is 9:1. 

The problem thus splits into five separate problems of binary classification of 24-character strings with an alphabet of 20 symbols. 


## The model

## Results

## Discussion

